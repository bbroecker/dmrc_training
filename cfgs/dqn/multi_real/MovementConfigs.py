class MovementConfig1(object):
    init_scale = 0.01
    init_constant = 0.0
    # init_scale = 1.0
    # max_learning_rate = 0.0008
    max_learning_rate = 0.0007
    # min_learning_rate = 0.0001
    min_learning_rate = 0.0007
    learning_rate_time = 10.0
    max_grad_norm = 5
    # state_count = 5
    trace_length = 1
    hidden_size = [100]
    max_epoch = 6
    # max_max_epoch = 39
    keep_prob = 0.99
    lr_decay = 0.8
    batch_size = 20
    use_tanh = [True]
    buffer_size = 50000
    forget_bias = 1.0
    use_fp16 = False
    valid_batch_size = 100
    epsilon_start = 0.20
    epsilon_end = 0.15
    epsilon_time = 30.0
    epsilon = 0.15
    gamma = 0.90
    store_every_ts = 2
    train_freq = 3
    clamp = 5.0
    log_folder = "./log/dqn/multi_real/batch/lr0.0007_tanh100__batch20_trace_length1_forget_bias1.0_init_value0.01_keep_prob0.99_AdamOptimizer_2/"
    weight_folder = "./weights_save/dqn/multi_real/batch/lr0.0007_tanh100__batch20_trace_length1_forget_bias1.0_init_value0.01_keep_prob0.99_AdamOptimizer_2/"
    pre_train_episodes = 2
    # optimizer = "GradientDescentOptimizer"
    optimizer = "AdamOptimizer"
    # optimizer = "RMSPropOptimizer"


class MovementConfig2(object):
    init_scale = 0.01
    init_constant = 0.0
    # init_scale = 1.0
    # max_learning_rate = 0.0008
    max_learning_rate = 0.0007
    # min_learning_rate = 0.0001
    min_learning_rate = 0.0007
    learning_rate_time = 10.0
    max_grad_norm = 5
    # state_count = 5
    trace_length = 1
    hidden_size_dense = [100]
    # max_epoch = 6
    # max_max_epoch = 39
    keep_prob = 0.99
    lr_decay = 0.8
    batch_size = 20
    use_tanh_dense = [True]
    buffer_size = 500000
    forget_bias = 1.0
    use_fp16 = False
    valid_batch_size = 100
    epsilon_start = 0.90
    epsilon_end = 0.20
    epsilon_time = 60.0
    epsilon = 0.15
    gamma = 0.90
    store_every_ts = 2
    train_freq = 3
    clamp = 5.0
    # vocab_size = 10000
    pre_train_episodes = 2
    # optimizer = "GradientDescentOptimizer"
    optimizer = "AdamOptimizer"
    # optimizer = "RMSPropOptimizer"
    log_folder = "./log/dqn/multi_real/batch/lr0.0007_tanh100__batch20_trace_length1_forget_bias1.0_init_value0.01_keep_prob0.99_AdamOptimizer_0/"
    weight_folder = "./weights_save/dqn/multi_real/batch/lr0.0007_tanh100__batch20_trace_length1_forget_bias1.0_init_value0.01_keep_prob0.99_AdamOptimizer_0/"
    # weight_folder = "./weights_save/rnn_cnn/angle_real/batch/lr0.001_tanh900__batch30_trace_length6_forget_bias0.0_init_value0.01_keep_prob1.0_AdamOptimizer_1/"
    # vocab_size = 10000
    # optimizer = "GradientDescentOptimizer"
    # optimizer = "RMSPropOptimize